<!DOCTYPE html>
<html xml:lang="zh-CN" lang="zh-CN">

<head>
        <link rel="canonical" href="https://clashmihomo.github.io/news/article-46015.htm" />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>在K8s上部署Redis 集群</title>
        <meta name="description" content="一、前言 架构原理：每个Master都可以拥有多个Slave。当Master下线后，Redis集群会从多个Slave中选举出一个新的Master作为替代，而旧Master重新上线后变成新Master的" />
        <link rel="icon" href="/assets/website/img/clashmihomo/favicon.ico" type="image/x-icon"/>

    <meta name="author" content="Clash Mihomo免费机场订阅节点官网">
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://clashmihomo.github.io/news/article-46015.htm" />
    <meta property="og:site_name" content="Clash Mihomo免费机场订阅节点官网" />
    <meta property="og:title" content="在K8s上部署Redis 集群" />
    <meta property="og:image" content="https://clashmihomo.github.io/uploads/20240920-1/93ab6bd14d9c3942e768887175668d13.webp" />
        <meta property="og:release_date" content="2025-01-25T09:53:21" />
    <meta property="og:updated_time" content="2025-01-25T09:53:21" />
        <meta property="og:description" content="一、前言 架构原理：每个Master都可以拥有多个Slave。当Master下线后，Redis集群会从多个Slave中选举出一个新的Master作为替代，而旧Master重新上线后变成新Master的" />
        
    <!-- fonts -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/bootstrap/css/bootstrap.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/animate/animate.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/fontawesome/css/all.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/jarallax/jarallax.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/jquery-magnific-popup/jquery.magnific-popup.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/nouislider/nouislider.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/nouislider/nouislider.pips.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/odometer/odometer.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/swiper/swiper.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/kitecx-icons/style.css">
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/tiny-slider/tiny-slider.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/reey-font/stylesheet.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/owl-carousel/owl.carousel.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/owl-carousel/owl.theme.default.min.css" />
    <link rel="stylesheet" href="/assets/website/js/frontend/clashmihomo/twentytwenty/twentytwenty.css" />
    <!-- template styles -->
    <link rel="stylesheet" href="/assets/website/css/clashmihomo/kitecx.css" />
    <link rel="stylesheet" href="/assets/website/css/clashmihomo/kitecx-responsive.css" />

    <meta name="applicable-device" content="pc,mobile" />
    <meta name="renderer" content="webkit" />
    <meta name="force-rendering" content="webkit" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta name="robots" content="max-image-preview:large" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="在K8s上部署Redis 集群">
    <meta name="format-detection" content="telephone=no">

    <link rel="dns-prefetch" href="https:/www.googletagmanager.com">
    <link rel="dns-prefetch" href="https://www.googleadservices.com">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link rel="dns-prefetch" href="https://pagead2.googlesyndication.com">
    <link rel="dns-prefetch" href="https://cm.g.doubleclick.net">
    
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZN7DJKT3D0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZN7DJKT3D0');
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body data-page="detail">
    <div class="preloader">
        <img class="preloader__image" width="60" src="assets/images/loader.png" alt="" />
    </div>
    <!-- /.preloader -->
    <div class="page-wrapper">
                <header class="main-header clearfix">
            <div class="container">
                <div class="main-menu-wrapper">
                    <div class="main-header__logo">
                                                <a href="/">
                            <span>Clash Mihomo</span>
                        </a>
                                            </div>
                    <div class="main-menu-wrapper__bottom">
                        <nav class="main-menu">
                            <div class="main-menu__inner">
                                <a href="#" class="mobile-nav__toggler"><i class="fa fa-bars"></i></a>
                                <ul class="main-menu__list">
                                                                        <li><a href="/">首页</a></li>
                                                                        <li><a href="/free-nodes/">免费节点</a></li>
                                                                        <li><a href="/paid-subscribe/">推荐机场</a></li>
                                                                        <li><a href="/news/">新闻资讯</a></li>
                                                                        <li><a href="#">关于</a></li>
                                    <li><a href="#">联系</a></li>
                                </ul>
                                
                            </div>
                        </nav>
                    </div>
                </div>
            </div>
        </header>
        <div class="stricky-header stricked-menu main-menu">
            <div class="sticky-header__content"></div><!-- /.sticky-header__content -->
        </div><!-- /.stricky-header -->
        <!--Page Header Start-->
        <section class="page-header" style="background-image: url(/assets/website/img/clashmihomo/backgrounds/page-header-bg.jpg);">
            <div class="page-header-border"></div>
            <div class="page-header-border page-header-border-two"></div>
            <div class="page-header-border page-header-border-three"></div>
            <div class="page-header-border page-header-border-four"></div>
            <div class="page-header-border page-header-border-five"></div>
            <div class="page-header-border page-header-border-six"></div>
            <div class="container">
                <div class="page-header__inner">
                    <h1>在K8s上部署Redis 集群</h1>
                    <ul class="thm-breadcrumb list-unstyled">
                        <li><a href="/">首页</a></li>
                        <li><span>/</span></li>
                        <li><a href="/news/">新闻资讯</a></li>
                        <li><span>/</span></li>
                        <li>正文</li>
                    </ul>
                </div>
            </div>
        </section>
        <!--Page Header End-->
        <!--Welcome One Start-->
        <section class="welcome-one py-5">
            <div class="container">
                <div class="row">
                    <div class="col-md-9">
                                        <input type="hidden" id="share-website-info" data-name="Clash Meta免费节点订阅站" data-url="https://clash-meta.github.io">
                  				  				  				<div id="content_views" class="markdown_views prism-atom-one-dark"> <p><strong>一、前言</strong><br /> 架构原理：每个Master都可以拥有多个Slave。当Master下线后，Redis集群会从多个Slave中选举出一个新的Master作为替代，而旧Master重新上线后变成新Master的Slave。</p> <p><strong>二、准备操作</strong><br /> 本次部署主要基于该项目：</p> <pre><code>https://github.com/zuxqoj/kubernetes-redis-cluster</code></pre> <p>其包含了两种部署Redis集群的方式：</p> <p>StatefulSet<br /> Service&amp;Deployment</p> <p>两种方式各有优劣，对于像Redis、Mongodb、Zookeeper等有状态的服务，使用StatefulSet是首选方式。本文将主要介绍如何使用StatefulSet进行Redis集群的部署。</p> <p><strong>三、StatefulSet简介</strong><br /> RC、Deployment、DaemonSet都是面向无状态的服务，它们所管理的Pod的IP、名字，启停顺序等都是随机的，而StatefulSet是什么？顾名思义，有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等。<br /> StatefulSet本质上是Deployment的一种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中，Pod名字称为网络标识(hostname)，还必须要用到共享存储。<br /><strong>在Deployment中，与之对应的服务是service，而在StatefulSet中与之对应的headless service，headless service，即无头服务，与service的区别就是它没有Cluster IP，解析它的名称时将返回该Headless Service对应的全部Pod的Endpoint列表。</strong><br /> 除此之外，StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod副本创建了一个DNS域名，这个域名的格式为：</p> <pre><code>$(podname).(headless server name)    FQDN： $(podname).(headless server name).namespace.svc.cluster.local</code></pre> <p>也即是说，对于有状态服务，我们最好使用固定的网络标识（如域名信息）来标记节点，当然这也需要应用程序的支持（如Zookeeper就支持在配置文件中写入主机域名）。<br /> StatefulSet基于Headless Service（即没有Cluster IP的Service）为Pod实现了稳定的网络标志（包括Pod的hostname和DNS Records），在Pod重新调度后也保持不变。同时，结合PV/PVC，StatefulSet可以实现稳定的持久化存储，就算Pod重新调度后，还是能访问到原先的持久化数据。<br /> 以下为使用StatefulSet部署Redis的架构，无论是Master还是Slave，都作为StatefulSet的一个副本，并且数据通过PV进行持久化，对外暴露为一个Service，接受客户端请求。</p> <p><strong>四、部署过程</strong><br /> 本文参考项目的README中，简要介绍了基于StatefulSet的Redis创建步骤：</p> <p>1.创建NFS存储<br /> 2.创建PV<br /> 3.创建PVC<br /> 4.创建Configmap<br /> 5.创建headless服务<br /> 6.创建Redis StatefulSet<br /> 7.初始化Redis集群</p> <p>这里，我将参考如上步骤，实践操作并详细介绍Redis集群的部署过程。文中会涉及到很多K8S的概念，希望大家能提前了解学习</p> <p><strong>1.创建NFS存储</strong><br /> 创建NFS存储主要是为了给Redis提供稳定的后端存储，当Redis的Pod重启或迁移后，依然能获得原先的数据。这里，我们先要创建NFS，然后通过使用PV为Redis挂载一个远程的NFS路径。</p> <p><strong>安装NFS</strong></p> <pre><code>yum -y install nfs-utils（主包提供文件系统） yum -y install rpcbind（提供rpc协议）</code></pre> <p>然后，新增/etc/exports文件，用于设置需要共享的路径：</p> <pre><code>[root@ftp pv3]# cat /etc/exports /usr/local/k8s/redis/pv1 192.168.0.0/24(rw,sync,no_root_squash) /usr/local/k8s/redis/pv2 192.168.0.0/24(rw,sync,no_root_squash) /usr/local/k8s/redis/pv3 192.168.0.0/24(rw,sync,no_root_squash) /usr/local/k8s/redis/pv4 192.168.0.0/24(rw,sync,no_root_squash) /usr/local/k8s/redis/pv5 192.168.0.0/24(rw,sync,no_root_squash) /usr/local/k8s/redis/pv6 192.168.0.0/24(rw,sync,no_root_squash)</code></pre> <p><img decoding="async" src="http://img.555519.xyz/uploads3/20220723/6345a2bc6b04e6c27ff8c94a68283b09.jpg" alt="在K8s上部署Redis 集群"><br /> 创建相应目录</p> <pre><code>[root@ftp quizii]# mkdir -p /usr/local/k8s/redis/pv{1..6}</code></pre> <p>接着，启动NFS和rpcbind服务：</p> <pre><code>systemctl restart rpcbind systemctl restart nfs systemctl enable nfs   [root@ftp pv3]# exportfs -v /usr/local/k8s/redis/pv1 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) /usr/local/k8s/redis/pv2 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) /usr/local/k8s/redis/pv3 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) /usr/local/k8s/redis/pv4 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) /usr/local/k8s/redis/pv5 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) /usr/local/k8s/redis/pv6 		192.168.0.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)</code></pre> <p>客户端</p> <pre><code>yum -y install nfs-utils</code></pre> <p>查看存储端共享</p> <pre><code>[root@node2 ~]# showmount -e 192.168.0.222 Export list for 192.168.0.222: /usr/local/k8s/redis/pv6 192.168.0.0/24 /usr/local/k8s/redis/pv5 192.168.0.0/24 /usr/local/k8s/redis/pv4 192.168.0.0/24 /usr/local/k8s/redis/pv3 192.168.0.0/24 /usr/local/k8s/redis/pv2 192.168.0.0/24 /usr/local/k8s/redis/pv1 192.168.0.0/24</code></pre> <p>创建PV<br /> 每一个Redis Pod都需要一个独立的PV来存储自己的数据，因此可以创建一个pv.yaml文件，包含6个PV：</p> <pre><code>[root@master redis]# cat pv.yaml  apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-pv1 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv1"  --- apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-vp2 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv2"  --- apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-pv3 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv3"  --- apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-pv4 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv4"  --- apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-pv5 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv5"  --- apiVersion: v1 kind: PersistentVolume metadata:   name: nfs-pv6 spec:   capacity:     storage: 200M   accessModes:     - ReadWriteMany   nfs:     server: 192.168.0.222     path: "/usr/local/k8s/redis/pv6"</code></pre> <p>如上，可以看到所有PV除了名称和挂载的路径外都基本一致。执行创建即可：</p> <pre><code>[root@master redis]#kubectl create -f pv.yaml  persistentvolume "nfs-pv1" created persistentvolume "nfs-pv2" created persistentvolume "nfs-pv3" created persistentvolume "nfs-pv4" created persistentvolume "nfs-pv5" created persistentvolume "nfs-pv6" created</code></pre> <p><strong>2.创建Configmap</strong><br /> 这里，我们可以直接将Redis的配置文件转化为Configmap，这是一种更方便的配置读取方式。配置文件redis.conf如下</p> <pre><code>[root@master redis]# cat redis.conf  appendonly yes cluster-enabled yes cluster-config-file /var/lib/redis/nodes.conf cluster-node-timeout 5000 dir /var/lib/redis port 6379</code></pre> <p>创建名为redis-conf的Configmap：</p> <pre><code>kubectl create configmap redis-conf --from-file=redis.conf</code></pre> <p>查看创建的configmap:</p> <pre><code>[root@master redis]# kubectl describe cm redis-conf Name:         redis-conf Namespace:    default Labels:       &lt;none&gt; Annotations:  &lt;none&gt;  Data ==== redis.conf: ---- appendonly yes cluster-enabled yes cluster-config-file /var/lib/redis/nodes.conf cluster-node-timeout 5000 dir /var/lib/redis port 6379  Events:  &lt;none&gt;</code></pre> <p>如上，redis.conf中的所有配置项都保存到redis-conf这个Configmap中。</p> <p><strong>3.创建Headless service</strong><br /> Headless service是StatefulSet实现稳定网络标识的基础，我们需要提前创建。准备文件headless-service.yml如下：</p> <pre><code>[root@master redis]# cat headless-service.yaml  apiVersion: v1 kind: Service metadata:   name: redis-service   labels:     app: redis spec:   ports:   - name: redis-port     port: 6379   clusterIP: None   selector:     app: redis     appCluster: redis-cluster</code></pre> <p>创建：</p> <pre><code>kubectl create -f headless-service.yml</code></pre> <p>查看：<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220723/2a061036249a51de704f41ddac83b9dc.jpg" alt="在K8s上部署Redis 集群"><br /> 可以看到，服务名称为redis-service，其CLUSTER-IP为None，表示这是一个“无头”服务。</p> <p><strong>4.创建Redis 集群节点</strong><br /> 创建好Headless service后，就可以利用StatefulSet创建Redis 集群节点，这也是本文的核心内容。我们先创建redis.yml文件：</p> <pre><code>[root@master redis]# cat redis.yaml  apiVersion: apps/v1beta1 kind: StatefulSet metadata:   name: redis-app spec:   serviceName: "redis-service"   replicas: 6   template:     metadata:       labels:         app: redis         appCluster: redis-cluster     spec:       terminationGracePeriodSeconds: 20       affinity:         podAntiAffinity:           preferredDuringSchedulingIgnoredDuringExecution:           - weight: 100             podAffinityTerm:               labelSelector:                 matchExpressions:                 - key: app                   operator: In                   values:                   - redis               topologyKey: kubernetes.io/hostname       containers:       - name: redis         image: redis         command:           - "redis-server"         args:           - "/etc/redis/redis.conf"           - "--protected-mode"           - "no"         resources:           requests:             cpu: "100m"             memory: "100Mi"         ports:             - name: redis               containerPort: 6379               protocol: "TCP"             - name: cluster               containerPort: 16379               protocol: "TCP"         volumeMounts:           - name: "redis-conf"             mountPath: "/etc/redis"           - name: "redis-data"             mountPath: "/var/lib/redis"       volumes:       - name: "redis-conf"         configMap:           name: "redis-conf"           items:             - key: "redis.conf"               path: "redis.conf"   volumeClaimTemplates:   - metadata:       name: redis-data     spec:       accessModes: [ "ReadWriteMany" ]       resources:         requests:           storage: 200M</code></pre> <p>如上，总共创建了6个Redis节点(Pod)，其中3个将用于master，另外3个分别作为master的slave；Redis的配置通过volume将之前生成的redis-conf这个Configmap，挂载到了容器的/etc/redis/redis.conf；Redis的数据存储路径使用volumeClaimTemplates声明（也就是PVC），其会绑定到我们先前创建的PV上。<br /> 这里有一个关键概念——Affinity，请参考官方文档详细了解。其中，podAntiAffinity表示反亲和性，其决定了某个pod不可以和哪些Pod部署在同一拓扑域，可以用于将一个服务的POD分散在不同的主机或者拓扑域中，提高服务本身的稳定性。<br /> 而PreferredDuringSchedulingIgnoredDuringExecution 则表示，在调度期间尽量满足亲和性或者反亲和性规则，如果不能满足规则，POD也有可能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。<br /> 在这里，matchExpressions规定了Redis Pod要尽量不要调度到包含app为redis的Node上，也即是说已经存在Redis的Node上尽量不要再分配Redis Pod了。但是，由于我们只有三个Node，而副本有6个，因此根据PreferredDuringSchedulingIgnoredDuringExecution，这些豌豆不得不得挤一挤，挤挤更健康~<br /> 另外，根据StatefulSet的规则，我们生成的Redis的6个Pod的hostname会被依次命名为<code>$(statefulset名称)-$(序号)</code> 如下图所示：</p> <pre><code>[root@master redis]# kubectl get pods -o wide  NAME                                            READY     STATUS      RESTARTS   AGE       IP             NODE            NOMINATED NODE redis-app-0                                     1/1       Running     0          2h        172.17.24.3    192.168.0.144   &lt;none&gt; redis-app-1                                     1/1       Running     0          2h        172.17.63.8    192.168.0.148   &lt;none&gt; redis-app-2                                     1/1       Running     0          2h        172.17.24.8    192.168.0.144   &lt;none&gt; redis-app-3                                     1/1       Running     0          2h        172.17.63.9    192.168.0.148   &lt;none&gt; redis-app-4                                     1/1       Running     0          2h        172.17.24.9    192.168.0.144   &lt;none&gt; redis-app-5                                     1/1       Running     0          2h        172.17.63.10   192.168.0.148   &lt;none&gt;</code></pre> <p>如上，可以看到这些Pods在部署时是以{0…N-1}的顺序依次创建的。注意，直到redis-app-0状态启动后达到Running状态之后，redis-app-1 才开始启动。<br /> 同时，每个Pod都会得到集群内的一个DNS域名，格式为<code>$(podname).$(service name).$(namespace).svc.cluster.local</code> ，也即是：</p> <pre><code>redis-app-0.redis-service.default.svc.cluster.local redis-app-1.redis-service.default.svc.cluster.local ...以此类推...</code></pre> <p>在K8S集群内部，这些Pod就可以利用该域名互相通信。我们可以使用busybox镜像的nslookup检验这些域名：</p> <pre><code>[root@master redis]# kubectl exec -ti busybox -- nslookup redis-app-0.redis-service Server:    10.0.0.2 Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local  Name:      redis-app-0.redis-service Address 1: 172.17.24.3</code></pre> <p><strong>可以看到， redis-app-0的IP为172.17.24.3。<em>当然，若Redis Pod迁移或是重启（我们可以手动删除掉一个Redis Pod来测试），IP是会改变的</em>，但是Pod的域名、SRV records、A record都不会改变。</strong></p> <p>另外可以发现，我们之前创建的pv都被成功绑定了：</p> <pre><code>[root@master redis]# kubectl get pv NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                            STORAGECLASS   REASON    AGE nfs-pv1   200M       RWX            Retain           Bound     default/redis-data-redis-app-2                            3h nfs-pv3   200M       RWX            Retain           Bound     default/redis-data-redis-app-4                            3h nfs-pv4   200M       RWX            Retain           Bound     default/redis-data-redis-app-5                            3h nfs-pv5   200M       RWX            Retain           Bound     default/redis-data-redis-app-1                            3h nfs-pv6   200M       RWX            Retain           Bound     default/redis-data-redis-app-0                            3h nfs-vp2   200M       RWX            Retain           Bound     default/redis-data-redis-app-3                            3h</code></pre> <p><strong>5.初始化Redis集群</strong></p> <p>创建好6个Redis Pod后，我们还需要利用常用的Redis-tribe工具进行集群的初始化</p> <p><strong>创建Ubuntu容器</strong><br /> 由于Redis集群必须在所有节点启动后才能进行初始化，而如果将初始化逻辑写入Statefulset中，则是一件非常复杂而且低效的行为。这里，本人不得不称赞一下原项目作者的思路，值得学习。也就是说，我们可以在K8S上创建一个额外的容器，专门用于进行K8S集群内部某些服务的管理控制。<br /> 这里，我们专门启动一个Ubuntu的容器，可以在该容器中安装Redis-tribe，进而初始化Redis集群，执行：</p> <pre><code>kubectl run -it ubuntu --image=ubuntu --restart=Never /bin/bash</code></pre> <p>我们使用阿里云的Ubuntu源，执行：</p> <pre><code>root@ubuntu:/# cat &gt; /etc/apt/sources.list &lt;&lt; EOF deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse  deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse  deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse  deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse   deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse &gt; EOF</code></pre> <p>成功后，原项目要求执行如下命令安装基本的软件环境：</p> <pre><code>apt-get update apt-get install -y vim wget python2.7 python-pip redis-tools dnsutils</code></pre> <p>初始化集群<br /> 首先，我们需要安装<code>redis-trib</code>：</p> <pre><code>pip install redis-trib==0.5.1</code></pre> <p>然后，创建只有Master节点的集群：</p> <pre><code>redis-trib.py create</code></pre> <p><code>dig +short redis-app-0.redis-service.default.svc.cluster.local</code>:6379<br /><code>dig +short redis-app-1.redis-service.default.svc.cluster.local</code>:6379<br /><code>dig +short redis-app-2.redis-service.default.svc.cluster.local</code>:6379</p> <p>其次，为每个Master添加Slave</p> <pre><code>redis-trib.py replicate</code></pre> <p>–master-addr<code>dig +short redis-app-0.redis-service.default.svc.cluster.local</code>:6379<br /> –slave-addr<code>dig +short redis-app-3.redis-service.default.svc.cluster.local</code>:6379</p> <pre><code>redis-trib.py replicate</code></pre> <p>–master-addr<code>dig +short redis-app-1.redis-service.default.svc.cluster.local</code>:6379<br /> –slave-addr<code>dig +short redis-app-4.redis-service.default.svc.cluster.local</code>:6379</p> <pre><code>redis-trib.py replicate</code></pre> <p>–master-addr<code>dig +short redis-app-2.redis-service.default.svc.cluster.local</code>:6379<br /> –slave-addr<code>dig +short redis-app-5.redis-service.default.svc.cluster.local</code>:6379</p> <p>至此，我们的Redis集群就真正创建完毕了，连到任意一个Redis Pod中检验一下：</p> <pre><code>[root@master redis]# kubectl exec -it redis-app-2 /bin/bash root@redis-app-2:/data# /usr/local/bin/redis-cli -c 127.0.0.1:6379&gt; cluster nodes 5d3e77f6131c6f272576530b23d1cd7592942eec 172.17.24.3:6379@16379 master - 0 1559628533000 1 connected 0-5461 a4b529c40a920da314c6c93d17dc603625d6412c 172.17.63.10:6379@16379 master - 0 1559628531670 6 connected 10923-16383 368971dc8916611a86577a8726e4f1f3a69c5eb7 172.17.24.9:6379@16379 slave 0025e6140f85cb243c60c214467b7e77bf819ae3 0 1559628533672 4 connected 0025e6140f85cb243c60c214467b7e77bf819ae3 172.17.63.8:6379@16379 master - 0 1559628533000 2 connected 5462-10922 6d5ee94b78b279e7d3c77a55437695662e8c039e 172.17.24.8:6379@16379 myself,slave a4b529c40a920da314c6c93d17dc603625d6412c 0 1559628532000 5 connected 2eb3e06ce914e0e285d6284c4df32573e318bc01 172.17.63.9:6379@16379 slave 5d3e77f6131c6f272576530b23d1cd7592942eec 0 1559628533000 3 connected 127.0.0.1:6379&gt; cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:6 cluster_stats_messages_ping_sent:14910 cluster_stats_messages_pong_sent:15139 cluster_stats_messages_sent:30049 cluster_stats_messages_ping_received:15139 cluster_stats_messages_pong_received:14910 cluster_stats_messages_received:30049 127.0.0.1:6379&gt;</code></pre> <p>另外，还可以在NFS上查看Redis挂载的数据：</p> <pre><code>[root@ftp pv3]# ll /usr/local/k8s/redis/pv3 total 12 -rw-r--r-- 1 root root  92 Jun  4 11:36 appendonly.aof -rw-r--r-- 1 root root 175 Jun  4 11:36 dump.rdb -rw-r--r-- 1 root root 794 Jun  4 11:49 nodes.conf</code></pre> <p><strong>6.创建用于访问Service</strong><br /> 前面我们创建了用于实现StatefulSet的Headless Service，但该Service没有Cluster Ip，因此不能用于外界访问。所以，我们还需要创建一个Service，专用于为Redis集群提供访问和负载均衡：</p> <pre><code>[root@master redis]# cat redis-access-service.yaml  apiVersion: v1 kind: Service metadata:   name: redis-access-service   labels:     app: redis spec:   ports:   - name: redis-port     protocol: "TCP"     port: 6379     targetPort: 6379   selector:     app: redis     appCluster: redis-cluster</code></pre> <p>如上，该Service名称为<code>redis-access-service</code>，在K8S集群中暴露6379端口，并且会对<code>labels name</code>为<code>app: redis</code>或<code>appCluster: redis-cluster</code>的pod进行负载均衡。</p> <p>创建后查看：</p> <pre><code>[root@master redis]#  kubectl get svc redis-access-service -o wide NAME                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE       SELECTOR redis-access-service   ClusterIP   10.0.0.64    &lt;none&gt;        6379/TCP   2h        app=redis,appCluster=redis-cluster</code></pre> <p>如上，在K8S集群中，所有应用都可以通过<code>10.0.0.64 :6379</code>来访问Redis集群。当然，为了方便测试，我们也可以为Service添加一个NodePort映射到物理机上，这里不再详细介绍。</p> <p><strong>五、测试主从切换</strong><br /> 在K8S上搭建完好Redis集群后，我们最关心的就是其原有的高可用机制是否正常。这里，我们可以任意挑选一个Master的Pod来测试集群的主从切换机制，如<code>redis-app-0</code>：</p> <pre><code>[root@master redis]# kubectl get pods redis-app-0 -o wide NAME          READY     STATUS    RESTARTS   AGE       IP            NODE            NOMINATED NODE redis-app-1   1/1       Running   0          3h        172.17.24.3   192.168.0.144   &lt;none&gt;</code></pre> <p>进入<code>redis-app-0</code>查看：</p> <pre><code>[root@master redis]# kubectl exec -it redis-app-0 /bin/bash root@redis-app-0:/data# /usr/local/bin/redis-cli -c 127.0.0.1:6379&gt; role 1) "master" 2) (integer) 13370 3) 1) 1) "172.17.63.9"       2) "6379"       3) "13370" 127.0.0.1:6379&gt;</code></pre> <p>如上可以看到，<code>app-0</code>为master，slave为<code>172.17.63.9</code>即<code>redis-app-3</code>。</p> <p>接着，我们手动删除<code>redis-app-0</code>：</p> <pre><code>[root@master redis]# kubectl delete pod redis-app-0 pod "redis-app-0" deleted [root@master redis]#  kubectl get pod redis-app-0 -o wide NAME          READY     STATUS    RESTARTS   AGE       IP            NODE            NOMINATED NODE redis-app-0   1/1       Running   0          4m        172.17.24.3   192.168.0.144   &lt;none&gt;</code></pre> <p>我们再进入<code>redis-app-0</code>内部查看：</p> <pre><code>[root@master redis]# kubectl exec -it redis-app-0 /bin/bash root@redis-app-0:/data# /usr/local/bin/redis-cli -c 127.0.0.1:6379&gt; role 1) "slave" 2) "172.17.63.9" 3) (integer) 6379 4) "connected" 5) (integer) 13958</code></pre> <p>如上，<code>redis-app-0</code>变成了slave，从属于它之前的从节点<code>172.17.63.9</code>即<code>redis-app-3</code>。</p> <p>六、疑问<br /> 至此，大家可能会疑惑，那为什么没有使用稳定的标志，Redis Pod也能正常进行故障转移呢？这涉及了Redis本身的机制。因为，Redis集群中每个节点都有自己的NodeId（保存在自动生成的nodes.conf中），并且该NodeId不会随着IP的变化和变化，这其实也是一种固定的网络标志。也就是说，就算某个Redis Pod重启了，该Pod依然会加载保存的NodeId来维持自己的身份。我们可以在NFS上查看redis-app-1的nodes.conf文件：</p> <p>[root@k8s-node2 ~]# cat /usr/local/k8s/redis/pv1/nodes.conf 96689f2018089173e528d3a71c4ef10af68ee462 192.168.169.209:6379@16379 slave d884c4971de9748f99b10d14678d864187a9e5d3 0 1526460952651 4 connected237d46046d9b75a6822f02523ab894928e2300e6 192.168.169.200:6379@16379 slave c15f378a604ee5b200f06cc23e9371cbc04f4559 0 1526460952651 1 connected<br /> c15f378a604ee5b200f06cc23e9371cbc04f4559 192.168.169.197:6379@16379 master - 0 1526460952651 1 connected 10923-16383d884c4971de9748f99b10d14678d864187a9e5d3 192.168.169.205:6379@16379 master - 0 1526460952651 4 connected 5462-10922c3b4ae23c80ffe31b7b34ef29dd6f8d73beaf85f 192.168.169.198:6379@16379 myself,slave c8a8f70b4c29333de6039c47b2f3453ed11fb5c2 0 1526460952565 3 connected<br /> c8a8f70b4c29333de6039c47b2f3453ed11fb5c2 192.168.169.201:6379@16379 master - 0 1526460952651 6 connected 0-5461vars currentEpoch 6 lastVoteEpoch 4<br /> 如上，第一列为NodeId，稳定不变；第二列为IP和端口信息，可能会改变。</p> <p>这里，我们介绍NodeId的两种使用场景：</p> <p>当某个Slave Pod断线重连后IP改变，但是Master发现其NodeId依旧， 就认为该Slave还是之前的Slave。</p> <p>当某个Master Pod下线后，集群在其Slave中选举重新的Master。待旧Master上线后，集群发现其NodeId依旧，会让旧Master变成新Master的slave。</p> <p>对于这两种场景，大家有兴趣的话还可以自行测试，注意要观察Redis的日志。</p> </div> 			                <div class="clearfix"></div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/news/article-45546.htm">Shell基础 — 10、Bash中的特殊符号详解</a></p>
                                        <p>下一个：<a href="/news/article-46016.htm">动物医疗机构的主管部门（动物医疗机构的主管部门是哪里）</a></p>
                                    </div>
                                    </div>
                    <div class="col-md-3">
                        <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/free-nodes/2024-12-23-node-share-links.htm" title="12月23日→20.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址">12月23日→20.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-11-11-mihomo-node-share.htm" title="11月11日→22.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址">11月11日→22.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-12-26-free-subscribe-node.htm" title="12月26日→20.9M/S|2024年最新免费节点Clash Mihomo订阅链接地址">12月26日→20.9M/S|2024年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/news/article-35185.htm" title="宠物用疫苗可以带上高铁吗北京地区（宠物疫苗能上地铁吗）">宠物用疫苗可以带上高铁吗北京地区（宠物疫苗能上地铁吗）</a></li>
                        <li class="py-2"><a href="/news/article-26228.htm" title="附近哪里有领养狗的地方 附近哪里有领养狗的地方呀">附近哪里有领养狗的地方 附近哪里有领养狗的地方呀</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-11-26-mihomo-node.htm" title="11月26日→18.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址">11月26日→18.4M/S|2024年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-1-3-mihomo-node-github.htm" title="1月3日→18.2M/S|2025年最新免费节点Clash Mihomo订阅链接地址">1月3日→18.2M/S|2025年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/news/article-24335.htm" title="教育管理专硕就业难痛哭(教育管理专硕就业前景)">教育管理专硕就业难痛哭(教育管理专硕就业前景)</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-11-10-free-subscribe-node.htm" title="11月10日→20.9M/S|2024年最新免费节点Clash Mihomo订阅链接地址">11月10日→20.9M/S|2024年最新免费节点Clash Mihomo订阅链接地址</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-1-15-mihomo-github.htm" title="1月15日→21.9M/S|2025年最新免费节点Clash Mihomo订阅链接地址">1月15日→21.9M/S|2025年最新免费节点Clash Mihomo订阅链接地址</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">81</span> <a href="/date/2025-01/" title="2025-01 归档">2025-01</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">93</span> <a href="/date/2024-12/" title="2024-12 归档">2024-12</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">34</span> <a href="/date/2024-11/" title="2024-11 归档">2024-11</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">25</span> <a href="/date/2024-10/" title="2024-10 归档">2024-10</a></h4>
            </li>
                    </ul>
    </div>
</div>

                    </div>
                </div>
            </div>
        </section>
        <!--Welcome One End-->
    </div><!-- /.page-wrapper -->
        <footer class="site-footer">
        <div class="site-footer-bottom">
            <div class="container">
                <div class="row">
                    <div class="col-xl-12">
                        <div class="site-footer-bottom__inner">
                            <div class="site-footer-bottom__left">
                            <p>
                                <a href="/">首页</a> | 
                                <a href="/free-node/">免费节点</a> | 
                                <a href="/news/">新闻资讯</a> |
                                <a href="/about-us.htm">关于我们</a> |
                                <a href="/disclaimer.htm">免责申明</a> |
                                <a href="/privacy.htm">隐私申明</a> |
                                <a href="/sitemap.xml">网站地图</a>
                            </p>
                                <p>
                                    <a href="/">Clash Mihomo免费机场订阅节点官网</a> 版权所有 Powered by WordPress
                                </p>
                            </div>
                            <ul class="site-footer-bottom__menu list-unstyled">
                                <li><a href="#">Privacy Policy</a></li>
                                <li><a href="#">Terms of Condition</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <div class="mobile-nav__wrapper">
        <div class="mobile-nav__overlay mobile-nav__toggler"></div>
        <!-- /.mobile-nav__overlay -->
        <div class="mobile-nav__content">
            <span class="mobile-nav__close mobile-nav__toggler"></span>
            <div class="logo-box">
                <a href="/" aria-label="logo image">Clash Mihomo</a>
            </div>
            <!-- /.logo-box -->
            <div class="mobile-nav__container"></div>
            <!-- /.mobile-nav__container -->
            <div class="mobile-nav__top">
                <div class="mobile-nav__social">
                    <a href="#" class="fab fa-twitter"></a>
                    <a href="#" class="fab fa-facebook-square"></a>
                    <a href="#" class="fab fa-pinterest-p"></a>
                    <a href="#" class="fab fa-instagram"></a>
                </div><!-- /.mobile-nav__social -->
            </div><!-- /.mobile-nav__top -->
        </div>
        <!-- /.mobile-nav__content -->
    </div>
    <!-- /.mobile-nav__wrapper -->
    <div class="side-menu__block">
        <div class="side-menu__block-overlay custom-cursor__overlay side-menu__toggler">
            <div class="cursor"></div>
            <div class="cursor-follower"></div>
        </div><!-- /.side-menu__block-overlay -->
        <div class="side-menu__block-inner ">
            <p class="side-menu__block__text site-footer__copy-text">Copyright &copy; 2021.Company name All rights reserved.<a target="_blank" href="https://sc.chinaz.com/moban/">&#x7F51;&#x9875;&#x6A21;&#x677F;</a></p>
        </div><!-- /.side-menu__block-inner -->
    </div><!-- /.side-menu__block -->
    <a href="#" data-target="html" class="scroll-to-target scroll-to-top"><i class="fa fa-angle-up"></i></a>
    <script src="/assets/website/js/frontend/clashmihomo/jquery/jquery-3.5.1.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jarallax/jarallax.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jquery-ajaxchimp/jquery.ajaxchimp.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jquery-appear/jquery.appear.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jquery-circle-progress/jquery.circle-progress.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jquery-magnific-popup/jquery.magnific-popup.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/jquery-validate/jquery.validate.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/nouislider/nouislider.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/odometer/odometer.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/swiper/swiper.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/tiny-slider/tiny-slider.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/wnumb/wNumb.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/wow/wow.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/isotope/isotope.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/countdown/countdown.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/owl-carousel/owl.carousel.min.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/twentytwenty/twentytwenty.js"></script>
    <script src="/assets/website/js/frontend/clashmihomo/twentytwenty/jquery.event.move.js"></script>
    <script src="http://ditu.google.cn/maps/api/js?key=AIzaSyATY4Rxc8jNvDpsK8ZetC7JyN4PFVYGCGM"></script>
    <!-- template js -->
    <script src="/assets/website/js/frontend/clashmihomo/kitecx.js"></script>
    <script src="https://www.freeclashnode.com/assets/js/frontend/invite-url.js"></script><script src="/assets/website/js/frontend/G.js"></script>
</body>

</html>